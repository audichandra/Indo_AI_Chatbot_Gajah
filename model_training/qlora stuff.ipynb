{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf76b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, load_metric\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoConfig, LlamaTokenizer, MistralConfig\n",
    "#from transformers import AdapterConfig, AdapterType\n",
    "from transformers import TrainingArguments, Trainer, BitsAndBytesConfig, DataCollatorWithPadding\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, hamming_loss, accuracy_score\n",
    "from peft import PeftModel, PeftConfig\n",
    "import sacrebleu\n",
    "import logging\n",
    "import bitsandbytes as bnb\n",
    "from peft import PeftModel\n",
    "from bitsandbytes.optim import GlobalOptimManager\n",
    "#from trl import SFTTrainer\n",
    "import optimum\n",
    "#import adapter-transformers\n",
    "from torch.nn import DataParallel\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6012a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"translated_final1.csv\") \n",
    "# Process the dataframe to fit the gpteacher format\n",
    "df['input'] = df[['flags', 'intent', 'category']].agg(', '.join, axis=1)\n",
    "df1 = df[['instruction', 'input', 'response']]\n",
    "df1.to_parquet('bitext.parquet', index=False)\n",
    "df2 = df1.sample(frac=0.1, random_state=42)\n",
    "train, temp = train_test_split(df2, test_size=0.3, random_state=42)\n",
    "val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "train.to_parquet('train.parquet', index=False)\n",
    "val.to_parquet('val.parquet', index=False)\n",
    "test.to_parquet('test.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Mistral model with custom configuration\n",
    "#config = MistralConfig()\n",
    "#model_id = \"Ichsan2895/Merak-7B-v4\"\n",
    "#model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# Load the pretrained weights into the model\n",
    "# Note: This step assumes that the custom configuration is compatible with the Merak model weights\n",
    "#model.load_state_dict(AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0, attn_implementation=\"flash_attention_2\", cache_dir=\"./model_cache\").state_dict())\n",
    "\n",
    "\n",
    "# Saving the model\n",
    "#model_save_path = \"/workspace/axolotl/merak_model\"\n",
    "#model.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a29366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8904a1adf3d45e9b77bc77941ff447a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
      "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "qlora_model = './axolotl/qlora-out'\n",
    "model_id = \"Ichsan2895/Merak-7B-v4\"\n",
    "#config = MistralConfig() \n",
    "#model1 = AutoModelForCausalLM.from_config(config)\n",
    "model1 = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0, attn_implementation=\"flash_attention_2\", cache_dir=\"/workspace\")\n",
    "model1 = PeftModel.from_pretrained(model1, qlora_model)\n",
    "model1 = model1.merge_and_unload()\n",
    "\n",
    "# Saving the model\n",
    "model_save_path1 = \"/workspace/axolotl/merged_model\"\n",
    "model1.save_pretrained(model_save_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe43c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165884b070194173ae289d0a34e56aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3810ab0b6803452caa568c55cd8a1221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ed3c5a6e2f4cb5a3e728580bd2c695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb4004eb0cb47b695bafbbe065ea920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbfc92bad124482bfbccb08c7ff5757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e8a933e6aa41dcb5c6c45c8637bb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saya mengerti bahwa Anda mencari bantuan dengan membatalkan pembelian Anda. Untuk membatalkan pembelian Anda, Anda dapat menghubungi tim pendukung pelanggan kami melalui situs web kami atau dengan menghubungi saluran dukungan pelanggan kami. Mereka akan dapat membimbing Anda melalui proses dan memastikan pembatalan yang halus. Jika Anda memiliki pertanyaan atau kekhawatiran lebih lanjut, jangan ragu untuk bertanya.\n"
     ]
    }
   ],
   "source": [
    "model_save_path1 = \"/workspace/axolotl/merged_model\"\n",
    "model1 = AutoModelForCausalLM.from_pretrained(model_save_path1, torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0, attn_implementation=\"flash_attention_2\")\n",
    "model_id = \"Ichsan2895/Merak-7B-v4\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def generate_response(question: str) -> str:\n",
    "    chat = [\n",
    "      {\"role\": \"system\", \"content\": \"Ada yang bisa saya bantu?\"},\n",
    "      {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model1.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"),\n",
    "                           attention_mask=inputs.attention_mask,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           pad_token_id=tokenizer.eos_token_id,\n",
    "                           max_new_tokens=256)\n",
    "        response = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
    "\n",
    "        assistant_start = f'''{question} \\n assistant\\n '''\n",
    "        response_start = response.find(assistant_start)\n",
    "        return response[response_start + len(assistant_start) :].strip()\n",
    "\n",
    "prompt = \"bagaimana saya dapat membatalkan pembelian saya?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c649d44e-56ba-4189-938d-31feec7c9ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9cd8f0b9a845c58f452cd47fc2ce18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/600 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576378824c6249ac86505d73f253c3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuk membatalkan pembelian Anda, Anda dapat mengikuti langkah-langkah ini:\n",
      "\n",
      "1. Buka aplikasi atau situs web Amazon.\n",
      "\n",
      "2. Klik pada ikon pengaturan di sudut kanan atas.\n",
      "\n",
      "3. Klik pada \"Pesan dan Pengiriman\" di bagian bawah halaman.\n",
      "\n",
      "4. Klik pada pesanan yang ingin Anda batalkan.\n",
      "\n",
      "5. Klik pada tombol \"Batalkan Pesanan\" di bagian atas halaman.\n",
      "\n",
      "6. Klik pada \"Batalkan Pesanan\" di dialog konfirmasi.\n",
      "\n",
      "7. Pilih alasan untuk membatalkan pesanan Anda.\n",
      "\n",
      "8. Klik pada \"Batalkan Pesanan\" untuk mengonfirmasi.\n",
      "\n",
      "Perhatikan bahwa jika Anda membatalkan pesanan Anda, Anda mungkin mungkin dikenakan\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Ichsan2895/Merak-7B-v4\"\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True, cache_dir=\"/workspace\")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def generate_response(question: str) -> str:\n",
    "    chat = [\n",
    "      {\"role\": \"system\", \"content\": \"Anda adalah Merak, sebuah model kecerdasan buatan yang dilatih oleh Muhammad Ichsan. Mohon jawab pertanyaan berikut dengan benar, faktual, dan ramah.\"},\n",
    "      {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"),\n",
    "                           attention_mask=inputs.attention_mask,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           pad_token_id=tokenizer.eos_token_id,\n",
    "                           max_new_tokens=256)\n",
    "        response = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
    "\n",
    "        assistant_start = f'''{question} \\n assistant\\n '''\n",
    "        response_start = response.find(assistant_start)\n",
    "        return response[response_start + len(assistant_start) :].strip()\n",
    "\n",
    "prompt = \"bagaimana saya dapat membatalkan pembelian saya?\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc5a593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a45b2389e54527afe392f276041c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Metrics:\n",
      "BLEU Score: 0.06850437696225994\n",
      "chrF Score: 49.27643486537506\n",
      "chrF++ Score: 45.75078773862919\n",
      "\n",
      "Test Set Metrics:\n",
      "BLEU Score: 0.062102389753086004\n",
      "chrF Score: 51.626813741243595\n",
      "chrF++ Score: 49.04707207666462\n"
     ]
    }
   ],
   "source": [
    "model_save_path1 = \"/workspace/axolotl/merged_model\"\n",
    "model1 = AutoModelForCausalLM.from_pretrained(model_save_path1, torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0, attn_implementation=\"flash_attention_2\")\n",
    "model_id = \"Ichsan2895/Merak-7B-v4\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id) \n",
    "\n",
    "# Function to generate responses for a list of questions\n",
    "def generate_responses(questions, model, tokenizer):\n",
    "    model.eval()\n",
    "    responses = []\n",
    "    for question in questions:\n",
    "        chat = [\n",
    "            {\"role\": \"system\", \"content\": \"Ada yang bisa saya bantu?\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=128, num_return_sequences=1)\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            assistant_start = f'''{question} \\n assistant\\n '''\n",
    "            response_start = response.find(assistant_start)\n",
    "            responses.append(response[response_start + len(assistant_start) :].strip())\n",
    "    return responses\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(df, model, tokenizer):\n",
    "    questions = df['instruction'].tolist()\n",
    "    generated_responses = generate_responses(questions, model, tokenizer)\n",
    "    references = [ref.split() for ref in df['response'].tolist()]\n",
    "    hypotheses = [resp.split() for resp in generated_responses]\n",
    "\n",
    "    bleu_score = corpus_bleu([[ref] for ref in references], hypotheses)\n",
    "    chrf_metric = sacrebleu.metrics.CHRF(word_order=0)\n",
    "    chrfpp_metric = sacrebleu.metrics.CHRF(word_order=2)\n",
    "\n",
    "    chrf_score = chrf_metric.corpus_score(generated_responses, [[ref] for ref in df['response'].tolist()])\n",
    "    chrfpp_score = chrfpp_metric.corpus_score(generated_responses, [[ref] for ref in df['response'].tolist()])\n",
    "\n",
    "    return bleu_score, chrf_score.score, chrfpp_score.score\n",
    "\n",
    "# Load the validation and test datasets\n",
    "val_df = pd.read_parquet('val.parquet')\n",
    "test_df = pd.read_parquet('test.parquet')\n",
    "\n",
    "# Compute metrics for validation and test datasets\n",
    "val_bleu, val_chrf, val_chrfpp = compute_metrics(val_df, model1, tokenizer)\n",
    "test_bleu, test_chrf, test_chrfpp = compute_metrics(test_df, model1, tokenizer)\n",
    "\n",
    "print(\"Validation Set Metrics:\")\n",
    "print(\"BLEU Score:\", val_bleu)\n",
    "print(\"chrF Score:\", val_chrf)\n",
    "print(\"chrF++ Score:\", val_chrfpp)\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"BLEU Score:\", test_bleu)\n",
    "print(\"chrF Score:\", test_chrf)\n",
    "print(\"chrF++ Score:\", test_chrfpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287b2892-ba0d-43ac-8265-f80a10211c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6254ed8558c402d858358125e9d066f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#BNB_CONFIG = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n",
    "model_save_path1 = \"/workspace/axolotl/merged_model\"\n",
    "model1 = AutoModelForCausalLM.from_pretrained(model_save_path1, torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0, attn_implementation=\"flash_attention_2\"\n",
    "                                             #, quantization_config=BNB_CONFIG \n",
    "                                             )\n",
    "model_id = \"Ichsan2895/Merak-7B-v4\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id) \n",
    "\n",
    "def generate_responses_in_batches(questions, model, tokenizer, batch_size=32):\n",
    "    print(f\"Using batch size: {batch_size}\")\n",
    "    model.eval()\n",
    "    responses = []\n",
    "\n",
    "    for i in range(0, len(questions), batch_size):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "\n",
    "        # Generate prompts for each question in the batch\n",
    "        batch_prompts = []\n",
    "        for question in batch_questions:\n",
    "            chat = [{\"role\": \"system\", \"content\": \"Ada yang bisa saya bantu?\"}, {\"role\": \"user\", \"content\": question}]\n",
    "            prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "            batch_prompts.append(prompt)\n",
    "\n",
    "        # Tokenize each prompt and collate them into a batch\n",
    "        batch_inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        batch_inputs = {k: v.to('cuda') for k, v in batch_inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = model.generate(**batch_inputs, max_new_tokens=128, num_return_sequences=1)\n",
    "            batch_responses = tokenizer.batch_decode(batch_outputs, skip_special_tokens=True)\n",
    "\n",
    "            for response in batch_responses:\n",
    "                assistant_start = \"assistant\\n\"  # Simplified start pattern\n",
    "                response_start = response.find(assistant_start)\n",
    "                responses.append(response[response_start + len(assistant_start):].strip())\n",
    "\n",
    "    return responses\n",
    "\n",
    "def compute_metrics1(df, model, tokenizer, batch_size=32):\n",
    "    questions = df['instruction'].tolist()\n",
    "    generated_responses = generate_responses_in_batches(questions, model, tokenizer, batch_size)\n",
    "    references = [ref.split() for ref in df['response'].tolist()]\n",
    "    hypotheses = [resp.split() for resp in generated_responses]\n",
    "\n",
    "    bleu_score = corpus_bleu([[ref] for ref in references], hypotheses)\n",
    "    chrf_metric = sacrebleu.metrics.CHRF(word_order=0)\n",
    "    chrfpp_metric = sacrebleu.metrics.CHRF(word_order=2)\n",
    "\n",
    "    chrf_score = chrf_metric.corpus_score(generated_responses, [[ref] for ref in df['response'].tolist()])\n",
    "    chrfpp_score = chrfpp_metric.corpus_score(generated_responses, [[ref] for ref in df['response'].tolist()])\n",
    "\n",
    "    return bleu_score, chrf_score.score, chrfpp_score.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44a855aa-73e8-4808-84ff-73d80727a5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch size: 32\n",
      "Validation Set Metrics:\n",
      "BLEU Score: 0.03390882789020255\n",
      "chrF Score: 49.27643486537506\n",
      "chrF++ Score: 45.75078773862919\n",
      "\n",
      "Test Set Metrics:\n",
      "BLEU Score: 0.034053665983892346\n",
      "chrF Score: 45.36736245520675\n",
      "chrF++ Score: 41.8263441561195\n"
     ]
    }
   ],
   "source": [
    "# Load the validation and test datasets\n",
    "val_df = pd.read_parquet('val.parquet')\n",
    "test_df = pd.read_parquet('test.parquet')\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32  # Adjust based on your GPU memory\n",
    "\n",
    "# Compute metrics for validation and test datasets\n",
    "val_bleu, val_chrf, val_chrfpp = compute_metrics1(val_df, model1, tokenizer, batch_size)\n",
    "test_bleu, test_chrf, test_chrfpp = compute_metrics1(test_df, model1, tokenizer, batch_size)\n",
    "\n",
    "print(\"Validation Set Metrics:\")\n",
    "print(\"BLEU Score:\", val_bleu)\n",
    "print(\"chrF Score:\", val_chrf)\n",
    "print(\"chrF++ Score:\", val_chrfpp)\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"BLEU Score:\", test_bleu)\n",
    "print(\"chrF Score:\", test_chrf)\n",
    "print(\"chrF++ Score:\", test_chrfpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path1 = \"/workspace/axolotl/merged_model\"\n",
    "model1 = AutoModelForCausalLM.from_pretrained(model_save_path1, torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0, attn_implementation=\"flash_attention_2\")\n",
    "model_id = \"Ichsan2895/Merak-7B-v4\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id) \n",
    "\n",
    "def generate_responses_in_batches(questions, model, tokenizer, batch_size=32):\n",
    "    print(f\"Using batch size: {batch_size}\")\n",
    "    model.eval()\n",
    "    responses = []\n",
    "\n",
    "    for i in range(0, len(questions), batch_size):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "\n",
    "        # Generate prompts and responses for each question in the batch\n",
    "        batch_responses = []\n",
    "        for question in batch_questions:\n",
    "            chat = [\n",
    "                {\"role\": \"system\", \"content\": \"Ada yang bisa saya bantu?\"},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ]\n",
    "            prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(**inputs, max_new_tokens=128, num_return_sequences=1)\n",
    "                response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "                responses.append(response)\n",
    "\n",
    "    return responses\n",
    "\n",
    "def compute_metrics1(df, model, tokenizer, batch_size=32):\n",
    "    questions = df['instruction'].tolist()\n",
    "    generated_responses = generate_responses_in_batches(questions, model, tokenizer, batch_size)\n",
    "    references = [ref.split() for ref in df['response'].tolist()]\n",
    "    hypotheses = [resp.split() for resp in generated_responses]\n",
    "\n",
    "    bleu_score = corpus_bleu([[ref] for ref in references], hypotheses)\n",
    "    chrf_metric = sacrebleu.metrics.CHRF(word_order=0)\n",
    "    chrfpp_metric = sacrebleu.metrics.CHRF(word_order=2)\n",
    "\n",
    "    chrf_score = chrf_metric.corpus_score(generated_responses, [[ref] for ref in df['response'].tolist()])\n",
    "    chrfpp_score = chrfpp_metric.corpus_score(generated_responses, [[ref] for ref in df['response'].tolist()])\n",
    "\n",
    "    return bleu_score, chrf_score.score, chrfpp_score.score\n",
    "\n",
    "# Load the validation and test datasets\n",
    "val_df = pd.read_parquet('val.parquet')\n",
    "test_df = pd.read_parquet('test.parquet')\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32  # Adjust based on your GPU memory\n",
    "\n",
    "# Compute metrics for validation and test datasets\n",
    "val_bleu, val_chrf, val_chrfpp = compute_metrics1(val_df, model1, tokenizer, batch_size)\n",
    "test_bleu, test_chrf, test_chrfpp = compute_metrics1(test_df, model1, tokenizer, batch_size)\n",
    "\n",
    "print(\"Validation Set Metrics:\")\n",
    "print(\"BLEU Score:\", val_bleu)\n",
    "print(\"chrF Score:\", val_chrf)\n",
    "print(\"chrF++ Score:\", val_chrfpp)\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"BLEU Score:\", test_bleu)\n",
    "print(\"chrF Score:\", test_chrf)\n",
    "print(\"chrF++ Score:\", test_chrfpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for BLEU and chrF evaluation\n",
    "test_questions = test[\"instruction\"].tolist()\n",
    "generated_responses = generate_responses(test_questions, model1, tokenizer, device)\n",
    "references = [ref.split() for ref in test[\"response\"].tolist()]\n",
    "hypotheses = [resp.split() for resp in generated_responses]\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = corpus_bleu([[ref] for ref in references], hypotheses)\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "\n",
    "# Calculate chrF score\n",
    "chrf = sacrebleu.metrics.CHRF(word_order=0)\n",
    "chrf_score = chrf.corpus_score(generated_responses, [[ref] for ref in test[\"response\"].tolist()])\n",
    "print(\"chrF Score:\", chrf_score.score)\n",
    "\n",
    "# Calculate chrF++ score\n",
    "chrfpp = sacrebleu.metrics.CHRF(word_order=2)\n",
    "chrfpp_score = chrfpp.corpus_score(generated_responses, [[ref] for ref in test[\"response\"].tolist()])\n",
    "print(\"chrF++ Score:\", chrfpp_score.score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ae00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ef629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
